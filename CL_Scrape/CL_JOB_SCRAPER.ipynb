{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling from Craigslist: Overview\n",
    "The following code scrapes Craig's List job posts. First, queries are run on each geographic specific CL site for each job category:\n",
    "\n",
    "Geo CL Site\n",
    "> `https://sfbay.craigslist.org`\n",
    "\n",
    "Job Categories\n",
    "> `ofc, bus, csr, edu, egr, etc...`\n",
    "\n",
    "The specifc URL's that we pull from CL return the search results page for full time jobs that were posted today for a given job category. An example that shows the proper URL format below:\n",
    "\n",
    "> `https://sfbay.craigslist.org/search/{Job Category}?employment_type=1&postedToday=1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import psycopg2\n",
    "%pylab inline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import re\n",
    "from __future__ import division\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def clean_name(raw):\n",
    "   letters=re.sub(\"[^a-zA-Z]\", \" \", raw)\n",
    "   words=letters.lower().split()\n",
    "   meaningful=words\n",
    "   return (\" \".join( meaningful))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sets of CL sites and job types to query over\n",
    "\n",
    "(1) SITES: The list of ~420 geo-specific CL sites was pulled from this website [find website] and are stored in a .csv file in the local directory. \n",
    "\n",
    "(2) JOBS TYPES: The list of job types are manually coded and were found by search each category on CL and finding the corresponding 3-letter code for the URL \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this code loads the various CL sites and job catagories we want to search\n",
    "\n",
    "CL_sites=pd.read_csv('CL_US_Sites.csv')\n",
    "CL_sites=CL_sites['site'].astype(str)\n",
    "random.shuffle(CL_sites)\n",
    "\n",
    "CL_job_cats=['ofc','bus','csr','edu','egr','etc','acc','fbh','lab','gov','hea','hum','eng','lgl','mnu','mar','med',\n",
    "            'npo','rej','ret','sls','spa','sci','sec','trd','sof','sad','tch','trp','tfr','web','wri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull all Job Posting from Today\n",
    "\n",
    "There are 4 nested loops that must be executed to pull all job postingson CL:\n",
    "\n",
    "(1) CL GEO SITE: Loop through all CL sites\n",
    "\n",
    "(2) JOB CATEGORY: Loop through all job categories\n",
    "    \n",
    "(3) RESULT PAGES: Since CL returns search results in batches of 100, we must iterate through each HTML result page and scrape it. \n",
    "\n",
    "(4) JOB DESCRIPTION: Loop through all job posting links to pull the text from the individual job descriptions\n",
    "\n",
    "The following batch of code will return a dataframe that contains the following:\n",
    "- Time of CL posting\n",
    "- CL posting title\n",
    "- CL link to the actual job posting\n",
    "- The job category\n",
    "- Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0673%\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "## To scrape, let this run for a few minutes and then hit 'stop'. Your IP address will be blocked eventually. \n",
    "\n",
    "results = []  # We'll store the data here\n",
    "\n",
    "# Careful with this...too many queries == your IP gets banned temporarily\n",
    "\n",
    "search_indices = np.arange(0, 100000, 100)  #picks first page of results, which is 100. NEED to change to grab all jobs 'posted today'\n",
    "\n",
    "# to loop through all (1) CL sites, (2) CL job cats, and (3) result pages\n",
    "\n",
    "site_job_count=0\n",
    "result_count=0\n",
    "desc_count=0\n",
    "\n",
    "results_df=pd.DataFrame(columns=['time','title', 'links', 'job description'])\n",
    "\n",
    "for site in CL_sites:\n",
    "    \n",
    "    for job_cat in CL_job_cats:\n",
    "        clear_output(wait=1)\n",
    "        print \"{:.4%}\".format(((site_job_count)/((len(CL_sites)*len(CL_job_cats)))))\n",
    "        site_job_count=site_job_count+1\n",
    "        \n",
    "        for i in search_indices:\n",
    "            result_count=result_count+1\n",
    "            #Create URL to pull\n",
    "            url = site + '/search/{0}?employment_type=1&postedToday=1'.format(job_cat)\n",
    "\n",
    "            try: #Send to CL.com\n",
    "                resp = requests.get(url, params={'s': i})\n",
    "\n",
    "                #Turn the returned info into readable HTML\n",
    "                txt = bs4(resp.text, 'html.parser')\n",
    "\n",
    "                #Grab just the listed job postings \n",
    "                jobs = txt.findAll(attrs={'class': \"row\"})\n",
    "\n",
    "                # Find the CL posting title, the posting link, and posting time\n",
    "                title = [rw.find('a', attrs={'class': 'hdrlnk'}).text\n",
    "                              for rw in jobs]\n",
    "                links = [rw.find('a', attrs={'class': 'hdrlnk'})['href']\n",
    "                         for rw in jobs]\n",
    "                time = [pd.to_datetime(rw.find('time')['datetime']) for rw in jobs]\n",
    "                \n",
    "                #links=np.unique(links)\n",
    "\n",
    "                #Create the full job posting URLs (the geo site + specifc sinlge posting)\n",
    "                temp=[]\n",
    "            \n",
    "                for x in links:\n",
    "                    if 'craigslist' in x: \n",
    "                        temp.append('http:' + x)\n",
    "                    else: \n",
    "                        temp.append((site + x))\n",
    "\n",
    "                #temp=set(links)-set(results_df.links)\n",
    "                temp_df=pd.DataFrame({'links':temp,'time':time,'title':title})\n",
    "                links_new=set(temp_df.links)-set(results_df.links)\n",
    "                results_df=pd.concat([results_df,temp_df],ignore_index=True)\n",
    "                results_df=results_df.drop_duplicates('links')\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #########Get individual CL job descriptions#########\n",
    "               \n",
    "                for link in links_new:\n",
    "                    url2 = link\n",
    "                    desc_count=desc_count+1\n",
    "                    try:\n",
    "                        resp2 = requests.get(url2)\n",
    "                        txt2 = bs4(resp2.text, 'html.parser')\n",
    "                        jobs2 = txt2.findAll('section', attrs={'class': 'userbody'})\n",
    "                        desc = [rw.get_text() for rw in jobs2]\n",
    "\n",
    "                        if len(desc)>0:\n",
    "                            results_df.loc[results_df.links==link,'job description']=clean_name(desc[0])\n",
    "                              \n",
    "                            \n",
    "                    except KeyboardInterrupt: \n",
    "                        sys.exit()\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "                # We'll create a dataframe to store all the data\n",
    "                                         \n",
    "                results_df.loc[results_df.links.isin(links_new),'job category'] = job_cat\n",
    "                results_df.loc[results_df.links.isin(links_new),'CL site'] =site\n",
    "                \n",
    "            except KeyboardInterrupt: \n",
    "                sys.exit()    \n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if len(jobs)<100:\n",
    "                    break\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Writes a csv file containing CL job postings from today\n",
    "\n",
    "results_df.to_csv('CL_JOBS.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
